# LLM Benchmark API - Environment Configuration
# Copy this file to .env and configure for your environment

#═══════════════════════════════════════════════════════════
# Server Configuration
#═══════════════════════════════════════════════════════════

# Server port (default: 8080)
PORT=8080

# Gin mode: debug, release, or test
# Use "release" for production deployments
GIN_MODE=debug

# Static file path for Vue.js frontend
# Relative or absolute path to the built frontend files
STATIC_PATH=client_dist

#═══════════════════════════════════════════════════════════
# CORS Configuration
#═══════════════════════════════════════════════════════════

# Allowed origins (comma-separated)
# Use "*" for development, specific origins for production
# Examples:
#   Development: *
#   Production: https://app.example.com,https://www.example.com
CORS_ALLOW_ORIGINS=*

# Allowed HTTP methods (comma-separated)
# Default: GET,POST,PUT,DELETE,OPTIONS,PATCH
# CORS_ALLOW_METHODS=GET,POST,OPTIONS

#═══════════════════════════════════════════════════════════
# Model Configuration - Option 1: Two Specific Models
#═══════════════════════════════════════════════════════════

# First Model Configuration
MODEL1_NAME=gpt-4
MODEL1_BASE_URL=https://api.openai.com/v1
MODEL1_API_KEY=sk-your-openai-key-here

# Second Model Configuration (optional - for comparisons)
MODEL2_NAME=claude-3-opus
MODEL2_BASE_URL=https://api.anthropic.com/v1
MODEL2_API_KEY=sk-ant-your-anthropic-key-here

#═══════════════════════════════════════════════════════════
# Model Configuration - Option 2: Generic Configuration
#═══════════════════════════════════════════════════════════

# Generic API configuration (fallback if MODEL1/MODEL2 not set)
# API_KEY=sk-your-api-key-here
# BASE_URL=https://api.openai.com/v1
# MODELS=gpt-4,gpt-3.5-turbo,claude-3-opus

#═══════════════════════════════════════════════════════════
# Cloud Foundry Configuration (Auto-detected)
#═══════════════════════════════════════════════════════════

# VCAP_SERVICES is automatically set by Cloud Foundry
# It contains service binding information for LLM models
# No manual configuration needed when running on Cloud Foundry

#═══════════════════════════════════════════════════════════
# Example Configurations
#═══════════════════════════════════════════════════════════

# Local Development:
# PORT=8080
# GIN_MODE=debug
# CORS_ALLOW_ORIGINS=*
# MODEL1_NAME=gpt-4
# MODEL1_BASE_URL=https://api.openai.com/v1
# MODEL1_API_KEY=sk-...

# Production:
# PORT=8080
# GIN_MODE=release
# CORS_ALLOW_ORIGINS=https://app.example.com
# MODEL1_NAME=gpt-4
# MODEL1_BASE_URL=https://api.openai.com/v1
# MODEL1_API_KEY=${SECRET_OPENAI_KEY}

# Cloud Foundry:
# PORT=${PORT}  # Auto-set by CF
# GIN_MODE=release
# STATIC_PATH=client/dist

